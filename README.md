# Do_an_ca_nhan_mon_AI
# 1. Giá»›i thiá»‡u bÃ i toÃ¡n 8_puzzle
BÃ i toÃ¡n 8-puzzle (hay cÃ²n gá»i lÃ  bÃ i toÃ¡n Ta-canh)

Má»™t báº£ng 3Ã—3 vá»›i cÃ¡c Ã´ trong Ä‘Ã³ cÃ³ sá»‘ tá»« 1 ->8 vÃ  1 Ã´ trá»‘ng, cÃ¡c Ã´ Ä‘Æ°á»£c Ä‘áº·t á»Ÿ cÃ¡c vá»‹ trÃ­ ngáº«u nhiÃªn, Ã´ trá»‘ng vÃ  Ã´ sá»‘ cÃ³ thá»ƒ Ä‘á»•i chá»— cho nhau, tÃ¬m cÃ¡ch di chuyá»ƒn cÃ¡c Ã´ sao cho cÃ¡c con sá»‘ vá» Ä‘Ãºng thá»© tá»±, bÃ i toÃ¡n Ä‘áº·t ra á»Ÿ Ä‘Ã¢y lÃ  tÃ¬m phÆ°Æ¡ng Ã¡n tá»‘i Æ°u sao cho sá»‘ láº§n di chuyá»ƒn lÃ  Ã­t nháº¥t.


# 2. Ná»™i dung

## 2.1. NhÃ³m thuáº­t toÃ¡n tÃ¬m kiáº¿m khÃ´ng cÃ³ thÃ´ng tin (Uninformed Search Algorithms)
CÃ¡c thÃ nh pháº§n chÃ­nh cá»§a bÃ i toÃ¡n tÃ¬m kiáº¿m vÃ  giáº£i phÃ¡p

+ Tráº¡ng thÃ¡i ban Ä‘áº§u
    - Má»™t lÆ°á»›i 3x3 vá»›i 8 sá»‘ tá»« 1 Ä‘áº¿n 8 vÃ  má»™t Ã´ trá»‘ng (0), Ä‘áº¡i diá»‡n cho tráº¡ng thÃ¡i khá»Ÿi Ä‘áº§u cá»§a bÃ i toÃ¡n ([[1 2 3], [0 5 6], [4 7 8]]).
+ Tráº¡ng thÃ¡i má»¥c tiÃªu
    - LÆ°á»›i 3x3 vá»›i thá»© tá»± sá»‘ tá»« 1 Ä‘áº¿n 8 vÃ  Ã´ trá»‘ng á»Ÿ vá»‹ trÃ­ cuá»‘i cÃ¹ng ([[1 2 3], [4 5 6], [7 8 0]]).
+ KhÃ´ng gian tráº¡ng thÃ¡i
    - Táº­p há»£p táº¥t cáº£ cÃ¡c cáº¥u hÃ¬nh cÃ³ thá»ƒ cá»§a lÆ°á»›i 3x3 hay cÃ¡c cÃ¡ch sáº¯p xáº¿p cá»¥ thá»ƒ vá»‹ trÃ­ cÃ¡c Ã´.
+ HÃ nh Ä‘á»™ng
    - Di chuyá»ƒn Ã´ trá»‘ng lÃªn, xuá»‘ng, trÃ¡i, pháº£i Ä‘á»ƒ hoÃ¡n Ä‘á»•i vá»›i Ã´ sá»‘ liá»n ká».
+ Chi phÃ­
    - Má»—i bÆ°á»›c di chuyá»ƒn cÃ³ chi phÃ­ báº±ng 1, vÃ¬ bÃ i toÃ¡n Æ°u tiÃªn tÃ¬m Ä‘Æ°á»ng Ä‘i ngáº¯n nháº¥t.
+ Giáº£i phÃ¡p
    - DÃ£y cÃ¡c tráº¡ng thÃ¡i tá»« tráº¡ng thÃ¡i ban Ä‘áº§u Ä‘áº¿n tráº¡ng thÃ¡i má»¥c tiÃªu, Ä‘Æ°á»£c táº¡o ra bá»Ÿi cÃ¡c: Thuáº­t toÃ¡n tÃ¬m kiáº¿m khÃ´ng cÃ³ thÃ´ng tin (BFS, DFS, UCS, IDS); Thuáº­t toÃ¡n tÃ¬m kiáº¿m cÃ³ thÃ´ng tin (A*, Greedy, IDA*); Thuáº­t toÃ¡n tÃ¬m kiáº¿m cá»¥c bá»™ ( Local Beam, Simple Hill Climbing, 

### 2.1.1. BFS
BFS lÃ  thuáº­t toÃ¡n duyá»‡t Ä‘á»“ thá»‹ hoáº·c cÃ¢y theo chiá»u rá»™ng, tá»©c lÃ  nÃ³ sáº½ duyá»‡t háº¿t táº¥t cáº£ cÃ¡c Ä‘á»‰nh á»Ÿ má»™t má»©c (level) trÆ°á»›c khi chuyá»ƒn sang má»©c tiáº¿p theo.
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng trong BFS
  - HÃ ng Ä‘á»£i(Queue): dÃ¹ng Ä‘á»ƒ lÆ°u cÃ¡c Ä‘á»‰nh cáº§n duyá»‡t tiáº¿p theo.
  - Táº­p há»£p hoáº·c Ä‘Ã¡nh dáº¥u (visited): Ä‘á»ƒ trÃ¡nh duyá»‡t láº¡i Ä‘á»‰nh Ä‘Ã£ thÄƒm.
+ HÃ¬nh áº£nh minh há»a: 
  
![NhÃ³m 1](GIF1/1/BFS.gif)
  
### 2.1.2. DFS
DFS lÃ  thuáº­t toÃ¡n duyá»‡t Ä‘á»“ thá»‹ hoáº·c cÃ¢y theo chiá»u sÃ¢u â€“ tá»©c lÃ  nÃ³ sáº½ Ä‘i sÃ¢u theo tá»«ng nhÃ¡nh trÆ°á»›c khi quay láº¡i vÃ  duyá»‡t cÃ¡c nhÃ¡nh cÃ²n láº¡i.
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng trong DFS
  - NgÄƒn xáº¿p(Stack): DFS báº£n cháº¥t lÃ  Ä‘i sÃ¢u xuá»‘ng tá»«ng nhÃ¡nh â†’ sá»­ dá»¥ng ngÄƒn xáº¿p (stack) Ä‘á»ƒ lÆ°u cÃ¡c Ä‘á»‰nh chá» Ä‘Æ°á»£c khÃ¡m phÃ¡.
  - Táº­p há»£p cÃ¡c visited Ä‘á»ƒ trÃ¡nh láº·p vÃ´ háº¡n (Ä‘áº·c biá»‡t trong Ä‘á»“ thá»‹ cÃ³ chu trÃ¬nh), cáº§n lÆ°u láº¡i cÃ¡c Ä‘á»‰nh Ä‘Ã£ thÄƒm:
      - Set (táº­p há»£p): Dá»… sá»­ dá»¥ng, truy cáº­p nhanh O(1)
      - Máº£ng Ä‘Ã¡nh dáº¥u (boolean array): DÃ¹ng khi Ä‘á»‰nh lÃ  sá»‘ nguyÃªn (0 â†’ n-1)
+ HÃ¬nh áº£nh minh há»a:
  
  ![NhÃ³m 1](GIF1/1/DFS.gif)
        
### 2.1.3. IDS (Iterative Deepening Search)
IDS (Iterative Deepening Search â€“ TÃ¬m kiáº¿m má»Ÿ rá»™ng láº·p) lÃ  sá»± káº¿t há»£p giá»¯a DFS vÃ  BFS. NÃ³ thá»±c hiá»‡n nhiá»u láº§n DFS vá»›i Ä‘á»™ sÃ¢u giá»›i háº¡n, tÄƒng dáº§n theo tá»«ng bÆ°á»›c.
ÄÃ¢y lÃ  thuáº­t toÃ¡n tÃ¬m kiáº¿m theo chiá»u sÃ¢u cÃ³ giá»›i háº¡n, láº·p Ä‘i láº·p láº¡i vá»›i giá»›i háº¡n Ä‘á»™ sÃ¢u tÄƒng dáº§n (depth limit = 0 â†’ 1 â†’ 2 â†’ ...), cho Ä‘áº¿n khi tÃ¬m tháº¥y lá»i giáº£i.
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng trong IDS
  - Call Stack (ngÄƒn xáº¿p hÃ m): khi dÃ¹ng DFS Ä‘á»‡ quy trong má»—i vÃ²ng láº·p giá»›i háº¡n Ä‘á»™ sÃ¢u
  - Táº­p há»£p/array Ä‘Ã¡nh dáº¥u (visited): giÃºp trÃ¡nh láº·p Ä‘á»‰nh (náº¿u cáº§n) â€“ thÆ°á»ng dÃ¹ng trong Ä‘á»“ thá»‹
  - VÃ²ng láº·p ngoÃ i (for depth in range(...)): kiá»ƒm soÃ¡t giá»›i háº¡n Ä‘á»™ sÃ¢u cá»§a DFS
+ HÃ¬nh áº£nh minh há»a:

    ![NhÃ³m 1](GIF1/1/IDS.gif)

  
    
### 2.1.4. UCS (Uniform-Cost Search)
UCS (Uniform Cost Search â€“ TÃ¬m kiáº¿m theo chi phÃ­ Ä‘á»“ng nháº¥t) lÃ  má»™t thuáº­t toÃ¡n tÃ¬m kiáº¿m trÃªn Ä‘á»“ thá»‹/cÃ¢y giá»‘ng nhÆ° BFS, nhÆ°ng thay vÃ¬ duyá»‡t theo má»©c, nÃ³ duyá»‡t theo chi phÃ­ Ä‘Æ°á»ng Ä‘i nhá» nháº¥t tá»« gá»‘c Ä‘áº¿n Ä‘á»‰nh hiá»‡n táº¡i.
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng trong UCS
  - Priority Queue (heap): luÃ´n chá»n Ä‘Æ°á»ng Ä‘i cÃ³ chi phÃ­ tháº¥p nháº¥t Ä‘á»ƒ má»Ÿ rá»™ng tiáº¿p theo
  - Táº­p visited / closed set: Ä‘Ã¡nh dáº¥u cÃ¡c Ä‘á»‰nh Ä‘Ã£ xá»­ lÃ½ Ä‘á»ƒ trÃ¡nh láº·p láº¡i
  - Danh sÃ¡ch ká»: biá»ƒu diá»…n Ä‘á»“ thá»‹ vÃ  chi phÃ­ cáº¡nh giá»¯a cÃ¡c Ä‘á»‰nh
  - Tuple (chi phÃ­, Ä‘á»‰nh): gÃ³i thÃ´ng tin cáº§n thiáº¿t cho priority queue

## 2.2. NhÃ³m thuáº­t toÃ¡n tÃ¬m kiáº¿m cÃ³ thÃ´ng tin (Informed Search Algorithms)
CÃ¡c thÃ nh pháº§n chÃ­nh cá»§a bÃ i toÃ¡n tÃ¬m kiáº¿m vÃ  giáº£i phÃ¡p

+ Tráº¡ng thÃ¡i ban Ä‘áº§u
    - Má»™t lÆ°á»›i 3x3 vá»›i 8 sá»‘ tá»« 1 Ä‘áº¿n 8 vÃ  má»™t Ã´ trá»‘ng (0), Ä‘áº¡i diá»‡n cho tráº¡ng thÃ¡i khá»Ÿi Ä‘áº§u cá»§a bÃ i toÃ¡n ([[1 2 3], [4 0 6], [7 5 8]]).
+ Tráº¡ng thÃ¡i má»¥c tiÃªu
    - LÆ°á»›i 3x3 vá»›i thá»© tá»± sá»‘ tá»« 1 Ä‘áº¿n 8 vÃ  Ã´ trá»‘ng á»Ÿ vá»‹ trÃ­ cuá»‘i cÃ¹ng ([[1 2 3], [4 5 6], [7 8 0]]).
+ KhÃ´ng gian tráº¡ng thÃ¡i
    - Táº­p há»£p táº¥t cáº£ cÃ¡c cáº¥u hÃ¬nh cÃ³ thá»ƒ cá»§a lÆ°á»›i 3x3 hay cÃ¡c cÃ¡ch sáº¯p xáº¿p cá»¥ thá»ƒ vá»‹ trÃ­ cÃ¡c Ã´.
+ HÃ nh Ä‘á»™ng
    - Di chuyá»ƒn Ã´ trá»‘ng lÃªn, xuá»‘ng, trÃ¡i, pháº£i Ä‘á»ƒ hoÃ¡n Ä‘á»•i vá»›i Ã´ sá»‘ liá»n ká».
+ Chi phÃ­
    - Má»—i bÆ°á»›c di chuyá»ƒn cÃ³ chi phÃ­ báº±ng 1, vÃ¬ bÃ i toÃ¡n Æ°u tiÃªn tÃ¬m Ä‘Æ°á»ng Ä‘i ngáº¯n nháº¥t.
+ Giáº£i phÃ¡p
    - DÃ£y cÃ¡c tráº¡ng thÃ¡i tá»« tráº¡ng thÃ¡i ban Ä‘áº§u Ä‘áº¿n tráº¡ng thÃ¡i má»¥c tiÃªu, Ä‘Æ°á»£c táº¡o ra bá»Ÿi cÃ¡c:  Thuáº­t toÃ¡n tÃ¬m kiáº¿m cÃ³ thÃ´ng tin (A*, Greedy, IDA*).
### 2.2.1. A*
Thuáº­t toÃ¡n A* (A star) lÃ  má»™t thuáº­t toÃ¡n tÃ¬m kiáº¿m theo Ä‘á»‹nh hÆ°á»›ng heuristic, Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i Ä‘á»ƒ tÃ¬m Ä‘Æ°á»ng Ä‘i ngáº¯n nháº¥t trong cÃ¡c bÃ i toÃ¡n nhÆ° báº£n Ä‘á»“, game, robot, AI, v.v. NÃ³ lÃ  sá»± káº¿t há»£p giá»¯a thuáº­t toÃ¡n Dijkstra (Æ°u tiÃªn Ä‘Æ°á»ng Ä‘i ngáº¯n nháº¥t) vÃ  thuáº­t toÃ¡n Greedy Best-First Search (Æ°u tiÃªn Ä‘iá»ƒm Ä‘áº¿n). Má»¥c tiÃªu cá»§a A* lÃ  tÃ¬m Ä‘Æ°á»ng Ä‘i ngáº¯n nháº¥t tá»« Ä‘iá»ƒm báº¯t Ä‘áº§u (start) Ä‘áº¿n Ä‘iá»ƒm Ä‘Ã­ch (goal), vá»›i chi phÃ­ tháº¥p nháº¥t. 
A* dÃ¹ng 1 hÃ m chi phÃ­ Ä‘Ã¡nh giÃ¡ tá»•ng quÃ¡t:
+ f(n)= g(n) + h(n)
+ Trong Ä‘Ã³:
  - g(n): chi phÃ­ thá»±c táº¿ tá»« Ä‘iá»ƒm báº¯t Ä‘áº§u Ä‘áº¿n nÃºt hiá»‡n táº¡i n.
  - h(n): chi phÃ­ Æ°á»›c lÆ°á»£ng tá»« nÃºt hiá»‡n táº¡i Ä‘áº¿n Ä‘Ã­ch (heuristic).
  - f(n): tá»•ng chi phÃ­ Æ°á»›c lÆ°á»£ng cá»§a hÃ nh trÃ¬nh Ä‘i qua nÃºt Ä‘Ã³.
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng trong A*:
  - Priority Queue (hÃ ng Ä‘á»£i Æ°u tiÃªn, thÆ°á»ng dÃ¹ng heapq trong Python): Ä‘á»ƒ chá»n nÃºt cÃ³ chi phÃ­ f(n) nhá» nháº¥t Ä‘á»ƒ má»Ÿ tiáº¿p.
  - Open set (thÆ°á»ng lÃ  má»™t priority queue): danh sÃ¡ch cÃ¡c nÃºt chá» xÃ©t tiáº¿p theo.
  - Closed set (thÆ°á»ng lÃ  set hoáº·c dict): danh sÃ¡ch cÃ¡c nÃºt Ä‘Ã£ xÃ©t, trÃ¡nh láº·p láº¡i.
  - Báº£n Ä‘á»“ cha (came_from) (dict): dÃ¹ng Ä‘á»ƒ truy váº¿t Ä‘Æ°á»ng Ä‘i sau khi tÃ¬m xong.
  - g_score (dict): lÆ°u chi phÃ­ thá»±c tá»« start Ä‘áº¿n tá»«ng nÃºt.
+ HÃ¬nh áº£nh minh há»a:
  
### 2.2.2. Greedy
Thuáº­t toÃ¡n Greedy (Tham lam) lÃ  má»™t chiáº¿n lÆ°á»£c giáº£i bÃ i toÃ¡n báº±ng cÃ¡ch luÃ´n chá»n lá»±a phÆ°Æ¡ng Ã¡n tá»‘i Æ°u nháº¥t táº¡i má»—i bÆ°á»›c vá»›i hy vá»ng ráº±ng tá»•ng thá»ƒ cÅ©ng sáº½ lÃ  tá»‘i Æ°u.
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng trong Greedy:
  - Danh sÃ¡ch hoáº·c máº£ng: LÆ°u cÃ¡c Ä‘á»‘i tÆ°á»£ng.
  - Sáº¯p xáº¿p (Sorting): Greedy thÆ°á»ng báº¯t Ä‘áº§u báº±ng viá»‡c sáº¯p xáº¿p theo má»™t tiÃªu chÃ­ (vÃ­ dá»¥: lá»£i nhuáº­n, trá»ng lÆ°á»£ng,...).
  - Priority Queue: Má»™t sá»‘ bÃ i toÃ¡n cáº§n truy xuáº¥t pháº§n tá»­ tá»‘t nháº¥t nhiá»u láº§n.
+ HÃ¬nh áº£nh minh há»a:
  
### 2.2.3. IDA*
IDA* (Iterative Deepening A*) lÃ  má»™t thuáº­t toÃ¡n káº¿t há»£p giá»¯a: DFS (Depth-First Search) â€“ Ä‘á»ƒ giáº£m bá»™ nhá»›, A* â€“ Ä‘á»ƒ Ä‘áº£m báº£o tÃ¬m kiáº¿m theo hÆ°á»›ng heuristic. NÃ³ thÆ°á»ng dÃ¹ng cho cÃ¡c bÃ i toÃ¡n cÃ³ khÃ´ng gian tráº¡ng thÃ¡i lá»›n nhÆ° 8-puzzle, 15-puzzle, Rubikâ€™s cube, nÆ¡i A* tá»‘n quÃ¡ nhiá»u bá»™ nhá»› vÃ¬ lÆ°u toÃ n bá»™ cÃ¢y tÃ¬m kiáº¿m.
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng trong IDA*:
  - Stack (há»‡ thá»‘ng gá»i Ä‘á»‡ quy): Ä‘á»ƒ thá»±c hiá»‡n DFS.
  - HÃ m heuristic h(n): thÆ°á»ng lÃ : Khoáº£ng cÃ¡ch Manhattan (cho 8-puzzle) vÃ  Hamming distance.
  - Visited path (táº¡m thá»i): Ä‘á»ƒ trÃ¡nh quay lui trong DFS.
+ HÃ¬nh áº£nh minh hoa:



## 2.3. NhÃ³m thuáº­t toÃ¡n tÃ¬m kiáº¿m cá»¥c bá»™ (Local Optimization Algorithms)
CÃ¡c thÃ nh pháº§n chÃ­nh cá»§a bÃ i toÃ¡n tÃ¬m kiáº¿m vÃ  giáº£i phÃ¡p

+ Tráº¡ng thÃ¡i ban Ä‘áº§u
    - Má»™t lÆ°á»›i 3x3 vá»›i 8 sá»‘ tá»« 1 Ä‘áº¿n 8 vÃ  má»™t Ã´ trá»‘ng (0), Ä‘áº¡i diá»‡n cho tráº¡ng thÃ¡i khá»Ÿi Ä‘áº§u cá»§a bÃ i toÃ¡n ([[1 2 3], [4 0 5], [7 5 8]]).
+ Tráº¡ng thÃ¡i má»¥c tiÃªu
    - LÆ°á»›i 3x3 vá»›i thá»© tá»± sá»‘ tá»« 1 Ä‘áº¿n 8 vÃ  Ã´ trá»‘ng á»Ÿ vá»‹ trÃ­ cuá»‘i cÃ¹ng ([[1 2 3], [4 5 6], [7 8 0]]).
+ KhÃ´ng gian tráº¡ng thÃ¡i
    - Táº­p há»£p táº¥t cáº£ cÃ¡c cáº¥u hÃ¬nh cÃ³ thá»ƒ cá»§a lÆ°á»›i 3x3 hay cÃ¡c cÃ¡ch sáº¯p xáº¿p cá»¥ thá»ƒ vá»‹ trÃ­ cÃ¡c Ã´.
+ HÃ nh Ä‘á»™ng
    - Di chuyá»ƒn Ã´ trá»‘ng lÃªn, xuá»‘ng, trÃ¡i, pháº£i Ä‘á»ƒ hoÃ¡n Ä‘á»•i vá»›i Ã´ sá»‘ liá»n ká».
+ Chi phÃ­
    - Má»—i bÆ°á»›c di chuyá»ƒn cÃ³ chi phÃ­ báº±ng 1, vÃ¬ bÃ i toÃ¡n Æ°u tiÃªn tÃ¬m Ä‘Æ°á»ng Ä‘i ngáº¯n nháº¥t.
+ Giáº£i phÃ¡p
    - DÃ£y cÃ¡c tráº¡ng thÃ¡i tá»« tráº¡ng thÃ¡i ban Ä‘áº§u Ä‘áº¿n tráº¡ng thÃ¡i má»¥c tiÃªu, Ä‘Æ°á»£c táº¡o ra bá»Ÿi cÃ¡c: Thuáº­t toÃ¡n tÃ¬m kiáº¿m cá»¥c bá»™ (Simple Hill Climbing, Steepest-Ascent hill climbing, Beam search, Stochastic hill climbing).

### 2.5.1. Simple hill climbing
Hill Climbing lÃ  má»™t thuáº­t toÃ¡n tÃ¬m kiáº¿m theo hÆ°á»›ng (heuristic). NÃ³ liÃªn tá»¥c di chuyá»ƒn theo hÆ°á»›ng tÄƒng dáº§n cá»§a giÃ¡ trá»‹ Ä‘Ã¡nh giÃ¡ (hÃ m heuristic) â€” giá»‘ng nhÆ° ngÆ°á»i leo nÃºi chá»‰ nhÃ¬n tháº¥y chá»— cao hÆ¡n hiá»‡n táº¡i vÃ  luÃ´n cá»‘ leo lÃªn Ä‘Ã³.
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng trong Simple hill climbing:
  - State: Ä‘áº¡i diá»‡n cho tráº¡ng thÃ¡i hiá»‡n táº¡i.
  - Heuristic function (h(n)): Ä‘Ã¡nh giÃ¡ "Ä‘á»™ tá»‘t" cá»§a tráº¡ng thÃ¡i.
  - Neighbor generator: sinh tráº¡ng thÃ¡i lÃ¢n cáº­n.
+ HÃ¬nh áº£nh minh há»a:
### 2.5.2. Steepest-Ascent hill climbing
Steepest-Ascent Hill Climbing lÃ  phiÃªn báº£n cáº£i tiáº¿n cá»§a Simple Hill Climbing. Thay vÃ¬ chá»‰ chá»n má»™t hÃ ng xÃ³m báº¥t ká»³ tá»‘t hÆ¡n tráº¡ng thÃ¡i hiá»‡n táº¡i, thuáº­t toÃ¡n xÃ©t táº¥t cáº£ cÃ¡c hÃ ng xÃ³m vÃ  chá»n hÃ ng xÃ³m tá»‘t nháº¥t (tá»©c cÃ³ giÃ¡ trá»‹ heuristic cao nháº¥t â€“ hoáº·c tháº¥p nháº¥t náº¿u Ä‘ang tÃ¬m giÃ¡ trá»‹ nhá» nháº¥t).
+ Cáº¥u trÃºc dá»¯ liá»‡u cá»§a Steepest-Ascent hill climbing:
  - State: Ä‘áº¡i diá»‡n tráº¡ng thÃ¡i hiá»‡n táº¡i.
  - Heuristic function h(n): Ä‘Ã¡nh giÃ¡ "Ä‘á»™ tá»‘t" cá»§a má»™t tráº¡ng thÃ¡i.
  - List of neighbors: Ä‘á»ƒ duyá»‡t táº¥t cáº£ hÃ ng xÃ³m.
+ HÃ¬nh áº£nh minh há»a:
### 2.5.3. Beam search
Beam Search lÃ  má»™t thuáº­t toÃ¡n tÃ¬m kiáº¿m heuristic giá»‘ng nhÆ° BFS káº¿t há»£p vá»›i A*, nhÆ°ng giá»›i háº¡n sá»‘ lÆ°á»£ng nhÃ¡nh Ä‘Æ°á»£c má»Ÿ rá»™ng táº¡i má»—i bÆ°á»›c Ä‘á»ƒ tiáº¿t kiá»‡m tÃ i nguyÃªn.
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng trong Beam search
  - List hÃ ng Ä‘á»£i hiá»‡n táº¡i (current_level) â€“ tráº¡ng thÃ¡i á»Ÿ bÆ°á»›c hiá»‡n táº¡i.
  - Danh sÃ¡ch cÃ¡c hÃ ng xÃ³m â€“ sinh tá»« cÃ¡c tráº¡ng thÃ¡i trong current_level.
  - HÃ m heuristic h(n) â€“ Ä‘Ã¡nh giÃ¡ Ä‘á»™ tá»‘t cá»§a tráº¡ng thÃ¡i.
  - Beam width (k) â€“ sá»‘ tráº¡ng thÃ¡i tá»‘t nháº¥t giá»¯ láº¡i á»Ÿ má»—i bÆ°á»›c.
+ HÃ¬nh áº£nh minh há»a:
### 2.5.4. Stochastic hill climbing
Stochastic Hill Climbing lÃ  má»™t biáº¿n thá»ƒ cá»§a thuáº­t toÃ¡n hill climbing, trong Ä‘Ã³ khÃ´ng chá»n luÃ´n hÃ ng xÃ³m tá»‘t nháº¥t, mÃ  chá»n ngáº«u nhiÃªn má»™t hÃ ng xÃ³m tá»‘t hÆ¡n hiá»‡n táº¡i. Äiá»u nÃ y giÃºp trÃ¡nh bá»‹ káº¹t trong local maximum vÃ  vÃ¹ng plateau.
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng trong Stochastic hill climbing
  - State: tráº¡ng thÃ¡i hiá»‡n táº¡i.
  - Heuristic function h(n): Ä‘Ã¡nh giÃ¡ Ä‘á»™ tá»‘t cá»§a tráº¡ng thÃ¡i.
  - Danh sÃ¡ch cÃ¡c hÃ ng xÃ³m: Ä‘á»ƒ chá»n ra hÃ ng xÃ³m tá»‘t hÆ¡n.
  - random.choice(): chá»n ngáº«u nhiÃªn 1 hÃ ng xÃ³m tá»‘t.
+ HÃ¬nh áº£nh minh há»a:

## 2.4. NhÃ³m thuáº­t toÃ¡n tÃ¬m kiáº¿m trong mÃ´i trÆ°á»ng phá»©c táº¡p (Search in complex environments)
CÃ¡c thÃ nh pháº§n chÃ­nh cá»§a bÃ i toÃ¡n tÃ¬m kiáº¿m vÃ  giáº£i phÃ¡p

+ Tráº¡ng thÃ¡i ban Ä‘áº§u
    - Má»™t lÆ°á»›i 3x3 vá»›i 8 sá»‘ tá»« 1 Ä‘áº¿n 8 vÃ  má»™t Ã´ trá»‘ng (0), Ä‘áº¡i diá»‡n cho tráº¡ng thÃ¡i khá»Ÿi Ä‘áº§u cá»§a bÃ i toÃ¡n ([[1 2 3], [4 0 5], [7 5 8]]).
+ Tráº¡ng thÃ¡i má»¥c tiÃªu
    - LÆ°á»›i 3x3 vá»›i thá»© tá»± sá»‘ tá»« 1 Ä‘áº¿n 8 vÃ  Ã´ trá»‘ng á»Ÿ vá»‹ trÃ­ cuá»‘i cÃ¹ng ([[1 2 3], [4 5 6], [7 8 0]]).
+ KhÃ´ng gian tráº¡ng thÃ¡i
    - Táº­p há»£p táº¥t cáº£ cÃ¡c cáº¥u hÃ¬nh cÃ³ thá»ƒ cá»§a lÆ°á»›i 3x3 hay cÃ¡c cÃ¡ch sáº¯p xáº¿p cá»¥ thá»ƒ vá»‹ trÃ­ cÃ¡c Ã´.
+ HÃ nh Ä‘á»™ng
    - Di chuyá»ƒn Ã´ trá»‘ng lÃªn, xuá»‘ng, trÃ¡i, pháº£i Ä‘á»ƒ hoÃ¡n Ä‘á»•i vá»›i Ã´ sá»‘ liá»n ká».
+ Chi phÃ­
    - Má»—i bÆ°á»›c di chuyá»ƒn cÃ³ chi phÃ­ báº±ng 1, vÃ¬ bÃ i toÃ¡n Æ°u tiÃªn tÃ¬m Ä‘Æ°á»ng Ä‘i ngáº¯n nháº¥t.
+ Giáº£i phÃ¡p
    - DÃ£y cÃ¡c tráº¡ng thÃ¡i tá»« tráº¡ng thÃ¡i ban Ä‘áº§u Ä‘áº¿n tráº¡ng thÃ¡i má»¥c tiÃªu, Ä‘Æ°á»£c táº¡o ra bá»Ÿi cÃ¡c: Thuáº­t toÃ¡n tÃ¬m kiáº¿m cá»¥c bá»™ (Search with no observation, Search with partically observation).
### 2.4.1. Search with no observation
Search with No Observation (TÃ¬m kiáº¿m khÃ´ng quan sÃ¡t) lÃ  má»™t dáº¡ng tÃ¬m kiáº¿m trong mÃ´i trÆ°á»ng khÃ´ng xÃ¡c Ä‘á»‹nh, trong Ä‘Ã³ agent (tÃ¡c nhÃ¢n) khÃ´ng thá»ƒ quan sÃ¡t tráº¡ng thÃ¡i hiá»‡n táº¡i cá»§a mÃ´i trÆ°á»ng sau má»—i hÃ nh Ä‘á»™ng.
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng trong Search with no observation
  - belief_state: Tráº¡ng thÃ¡i khÃ´ng cháº¯c cháº¯n hiá»‡n táº¡i.
  - frontier: CÃ¡c belief states Ä‘ang xÃ©t
  - explored: CÃ¡c belief states Ä‘Ã£ xÃ©t
  - transition model: Biá»ƒu diá»…n mÃ´ hÃ¬nh hÃ nh Ä‘á»™ng
  - goal_test: Kiá»ƒm tra belief state Ä‘áº¡t Ä‘Ã­ch
+ HÃ¬nh áº£nh minh há»a:


### 2.4.2. Search with partically observation
Thuáº­t toÃ¡n Search with partially observable (TÃ¬m kiáº¿m vá»›i quan sÃ¡t má»™t pháº§n) Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c bÃ i toÃ¡n trong Ä‘Ã³ tÃ¡c nhÃ¢n (agent) khÃ´ng thá»ƒ quan sÃ¡t toÃ n bá»™ tráº¡ng thÃ¡i cá»§a mÃ´i trÆ°á»ng. Thay vÃ o Ä‘Ã³, tÃ¡c nhÃ¢n chá»‰ cÃ³ thá»ƒ quan sÃ¡t má»™t pháº§n cá»§a mÃ´i trÆ°á»ng, Ä‘iá»u nÃ y dáº«n Ä‘áº¿n cÃ¡c váº¥n Ä‘á» vá» khÃ´ng Ä‘áº§y Ä‘á»§ thÃ´ng tin. Thuáº­t toÃ¡n nÃ y thÆ°á»ng Ä‘Æ°á»£c Ã¡p dá»¥ng trong cÃ¡c trÃ² chÆ¡i, robot tá»± hÃ nh, hoáº·c cÃ¡c há»‡ thá»‘ng thÃ´ng minh nÆ¡i tÃ¡c nhÃ¢n pháº£i Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh dá»±a trÃªn thÃ´ng tin háº¡n cháº¿.
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng trong Search with partically observation
  - Lá»‹ch sá»­ quan sÃ¡t (Observation History): LÆ°u trá»¯ cÃ¡c quan sÃ¡t Ä‘Ã£ nháº­n Ä‘Æ°á»£c theo thá»i gian. Cáº¥u trÃºc nÃ y giÃºp theo dÃµi cÃ¡c thÃ´ng tin mÃ  tÃ¡c nhÃ¢n Ä‘Ã£ quan sÃ¡t.
  - Belief State: Máº£ng hoáº·c tá»« Ä‘iá»ƒn (dict) lÆ°u trá»¯ xÃ¡c suáº¥t cá»§a cÃ¡c tráº¡ng thÃ¡i cÃ³ thá»ƒ cÃ³, Æ°á»›c lÆ°á»£ng tá»« cÃ¡c quan sÃ¡t Ä‘Ã£ nháº­n Ä‘Æ°á»£c.
  - Queue hoáº·c Stack: DÃ¹ng cho cÃ¡c thuáº­t toÃ¡n tÃ¬m kiáº¿m nhÆ° BFS (queue) hoáº·c DFS (stack) Ä‘á»ƒ quáº£n lÃ½ cÃ¡c tráº¡ng thÃ¡i cáº§n khÃ¡m phÃ¡.
  - *Priority Queue (cho A)**: HÃ ng Ä‘á»£i Æ°u tiÃªn giÃºp tá»• chá»©c cÃ¡c tráº¡ng thÃ¡i dá»±a trÃªn chi phÃ­ Æ°á»›c tÃ­nh, cho phÃ©p lá»±a chá»n tráº¡ng thÃ¡i tá»‘t nháº¥t Ä‘á»ƒ má»Ÿ rá»™ng tiáº¿p.
  - HashMap hoáº·c Dict: LÆ°u trá»¯ cÃ¡c tráº¡ng thÃ¡i hoáº·c káº¿t quáº£ trung gian trong quÃ¡ trÃ¬nh tÃ¬m kiáº¿m, há»— trá»£ tra cá»©u nhanh.
+ HÃ¬nh áº£nh minh há»a:


## 2.5. NhÃ³m thuáº­t toÃ¡n tÃ¬m kiáº¿m thá»a rÃ ng buá»™c (Constraint Satisfaction Problem)
CÃ¡c thÃ nh pháº§n chÃ­nh cá»§a bÃ i toÃ¡n tÃ¬m kiáº¿m vÃ  giáº£i phÃ¡p

+ Tráº¡ng thÃ¡i ban Ä‘áº§u
    - Má»™t lÆ°á»›i 3x3 vá»›i 8 sá»‘ tá»« 1 Ä‘áº¿n 8 vÃ  má»™t Ã´ trá»‘ng (0), Ä‘áº¡i diá»‡n cho tráº¡ng thÃ¡i khá»Ÿi Ä‘áº§u cá»§a bÃ i toÃ¡n ([[1 2 3], [4 0 5], [7 5 8]]).
+ Tráº¡ng thÃ¡i má»¥c tiÃªu
    - LÆ°á»›i 3x3 vá»›i thá»© tá»± sá»‘ tá»« 1 Ä‘áº¿n 8 vÃ  Ã´ trá»‘ng á»Ÿ vá»‹ trÃ­ cuá»‘i cÃ¹ng ([[1 2 3], [4 5 6], [7 8 0]]).
+ KhÃ´ng gian tráº¡ng thÃ¡i
    - Táº­p há»£p táº¥t cáº£ cÃ¡c cáº¥u hÃ¬nh cÃ³ thá»ƒ cá»§a lÆ°á»›i 3x3 hay cÃ¡c cÃ¡ch sáº¯p xáº¿p cá»¥ thá»ƒ vá»‹ trÃ­ cÃ¡c Ã´.
+ HÃ nh Ä‘á»™ng
    - Di chuyá»ƒn Ã´ trá»‘ng lÃªn, xuá»‘ng, trÃ¡i, pháº£i Ä‘á»ƒ hoÃ¡n Ä‘á»•i vá»›i Ã´ sá»‘ liá»n ká».
+ Chi phÃ­
    - Má»—i bÆ°á»›c di chuyá»ƒn cÃ³ chi phÃ­ báº±ng 1, vÃ¬ bÃ i toÃ¡n Æ°u tiÃªn tÃ¬m Ä‘Æ°á»ng Ä‘i ngáº¯n nháº¥t.
+ Giáº£i phÃ¡p
    - DÃ£y cÃ¡c tráº¡ng thÃ¡i tá»« tráº¡ng thÃ¡i ban Ä‘áº§u Ä‘áº¿n tráº¡ng thÃ¡i má»¥c tiÃªu, Ä‘Æ°á»£c táº¡o ra bá»Ÿi cÃ¡c: Thuáº­t toÃ¡n tÃ¬m kiáº¿m cá»¥c bá»™ (BackTracking, Min-Conflicts, Forward checking).
### 2.5.1. BackTracking
Backtracking lÃ  má»™t ká»¹ thuáº­t giáº£i quyáº¿t bÃ i toÃ¡n dá»±a trÃªn phÆ°Æ¡ng phÃ¡p thá»­ vÃ  sai, trong Ä‘Ã³ ta báº¯t Ä‘áº§u tá»« má»™t tráº¡ng thÃ¡i ban Ä‘áº§u, thá»­ nghiá»‡m cÃ¡c lá»±a chá»n khÃ¡c nhau, vÃ  quay láº¡i (backtrack) khi gáº·p pháº£i tÃ¬nh huá»‘ng khÃ´ng há»£p lá»‡ hoáº·c khÃ´ng thá»ƒ tiáº¿p tá»¥c Ä‘Æ°á»£c. Ká»¹ thuáº­t nÃ y Ä‘áº·c biá»‡t há»¯u Ã­ch trong cÃ¡c bÃ i toÃ¡n tá»‘i Æ°u, tÃ¬m kiáº¿m, hoáº·c cÃ¡c bÃ i toÃ¡n cÃ³ khÃ´ng gian tráº¡ng thÃ¡i lá»›n vÃ  cáº§n khÃ¡m phÃ¡ táº¥t cáº£ cÃ¡c kháº£ nÄƒng.
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng trong BackTracking
  - Danh sÃ¡ch hoáº·c Máº£ng: Äá»ƒ lÆ°u trá»¯ cÃ¡c bÆ°á»›c chá»n (cÃ¡c giÃ¡ trá»‹ hoáº·c tráº¡ng thÃ¡i táº¡m thá»i trong quÃ¡ trÃ¬nh giáº£i quyáº¿t bÃ i toÃ¡n).
  - Stack: DÃ¹ng Ä‘á»ƒ theo dÃµi cÃ¡c tráº¡ng thÃ¡i trong quÃ¡ trÃ¬nh thá»­ nghiá»‡m vÃ  quay láº¡i. Stack lÆ°u trá»¯ cÃ¡c quyáº¿t Ä‘á»‹nh Ä‘Ã£ thá»±c hiá»‡n.
  - Set hoáº·c HashSet: Äá»ƒ kiá»ƒm tra cÃ¡c Ä‘iá»u kiá»‡n rÃ ng buá»™c (cháº³ng háº¡n, kiá»ƒm tra sá»± trÃ¹ng láº·p cá»§a cÃ¡c quyáº¿t Ä‘á»‹nh).
+ HÃ¬nh áº£nh minh há»a:
### 2.5.2. Min-conflicts
Min-conflicts lÃ  má»™t thuáº­t toÃ¡n tÃ¬m kiáº¿m heuristic, thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n tá»‘i Æ°u hÃ³a, Ä‘áº·c biá»‡t lÃ  trong cÃ¡c bÃ i toÃ¡n cÃ³ rÃ ng buá»™c, nhÆ° bÃ i toÃ¡n n-Queens hoáº·c sáº¯p xáº¿p lá»‹ch. Ã tÆ°á»Ÿng chÃ­nh cá»§a thuáº­t toÃ¡n Min-conflicts lÃ  tÃ¬m cÃ¡ch giáº£m sá»‘ lÆ°á»£ng "xung Ä‘á»™t" (conflicts) trong má»—i bÆ°á»›c, thay vÃ¬ tÃ¬m kiáº¿m táº¥t cáº£ cÃ¡c tráº¡ng thÃ¡i hoáº·c thá»­ táº¥t cáº£ cÃ¡c kháº£ nÄƒng. Thuáº­t toÃ¡n nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  má»™t phÆ°Æ¡ng phÃ¡p local search, táº­p trung vÃ o viá»‡c cáº£i thiá»‡n dáº§n dáº§n báº±ng cÃ¡ch di chuyá»ƒn Ä‘áº¿n cÃ¡c tráº¡ng thÃ¡i vá»›i Ã­t xung Ä‘á»™t hÆ¡n.
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng trong Min-Conflicts
  - Danh sÃ¡ch/Máº£ng: DÃ¹ng Ä‘á»ƒ lÆ°u trá»¯ vá»‹ trÃ­ cá»§a cÃ¡c quÃ¢n háº­u (hoáº·c cÃ¡c quyáº¿t Ä‘á»‹nh trong cÃ¡c bÃ i toÃ¡n khÃ¡c).
  - Set hoáº·c Dictionary: DÃ¹ng Ä‘á»ƒ theo dÃµi cÃ¡c xung Ä‘á»™t trong quÃ¡ trÃ¬nh tÃ¬m kiáº¿m.
  - Heuristic Function: Äá»ƒ Ä‘Ã¡nh giÃ¡ xung Ä‘á»™t, cÃ³ thá»ƒ sá»­ dá»¥ng hÃ m tÃ­nh toÃ¡n sá»‘ lÆ°á»£ng xung Ä‘á»™t trong má»—i bÆ°á»›c.
+ HÃ¬nh áº£nh minh há»a:
### 2.5.3. Forward checking
Forward Checking lÃ  má»™t ká»¹ thuáº­t Ä‘Æ°á»£c sá»­ dá»¥ng trong giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n cÃ³ rÃ ng buá»™c (constraint satisfaction problems - CSPs), nhÆ° bÃ i toÃ¡n Ä‘á»• mÃ u Ä‘á»“ thá»‹, Sudoku, vÃ  cÃ¡c bÃ i toÃ¡n láº­p lá»‹ch. Ká»¹ thuáº­t nÃ y giÃºp tÄƒng hiá»‡u quáº£ cá»§a thuáº­t toÃ¡n báº±ng cÃ¡ch kiá»ƒm tra cÃ¡c rÃ ng buá»™c ngay khi lá»±a chá»n má»™t giÃ¡ trá»‹ cho biáº¿n, thay vÃ¬ Ä‘á»£i Ä‘áº¿n khi toÃ n bá»™ giáº£i phÃ¡p Ä‘Æ°á»£c xÃ¢y dá»±ng.
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng trong Forward checking
  - Dictionary: LÆ°u miá»n giÃ¡ trá»‹ cá»§a tá»«ng biáº¿n (domains[var] = [...]); LÆ°u rÃ ng buá»™c/lÃ¡ng giá»ng (neighbors[var] = [...])
  - List: LÆ°u danh sÃ¡ch biáº¿n, biáº¿n chÆ°a gÃ¡n.
  - Set: Quáº£n lÃ½ giÃ¡ trá»‹ kháº£ thi, kiá»ƒm tra vÃ  loáº¡i trÃ¹ng
  - Stack/Recursion	: Há»— trá»£ quay lui (backtrack) khi cáº§n
+ HÃ¬nh áº£nh minh há»a:

## 2.6. NhÃ³m thuáº­t toÃ¡n há»c tÄƒng cÆ°á»ng (Reinforcement Learning)
CÃ¡c thÃ nh pháº§n chÃ­nh cá»§a bÃ i toÃ¡n tÃ¬m kiáº¿m vÃ  giáº£i phÃ¡p

+ Tráº¡ng thÃ¡i ban Ä‘áº§u
    - Má»™t lÆ°á»›i 3x3 vá»›i 8 sá»‘ tá»« 1 Ä‘áº¿n 8 vÃ  má»™t Ã´ trá»‘ng (0), Ä‘áº¡i diá»‡n cho tráº¡ng thÃ¡i khá»Ÿi Ä‘áº§u cá»§a bÃ i toÃ¡n ([[1 2 3], [4 0 5], [7 5 8]]).
+ Tráº¡ng thÃ¡i má»¥c tiÃªu
    - LÆ°á»›i 3x3 vá»›i thá»© tá»± sá»‘ tá»« 1 Ä‘áº¿n 8 vÃ  Ã´ trá»‘ng á»Ÿ vá»‹ trÃ­ cuá»‘i cÃ¹ng ([[1 2 3], [4 5 6], [7 8 0]]).
+ KhÃ´ng gian tráº¡ng thÃ¡i
    - Táº­p há»£p táº¥t cáº£ cÃ¡c cáº¥u hÃ¬nh cÃ³ thá»ƒ cá»§a lÆ°á»›i 3x3 hay cÃ¡c cÃ¡ch sáº¯p xáº¿p cá»¥ thá»ƒ vá»‹ trÃ­ cÃ¡c Ã´.
+ HÃ nh Ä‘á»™ng
    - Di chuyá»ƒn Ã´ trá»‘ng lÃªn, xuá»‘ng, trÃ¡i, pháº£i Ä‘á»ƒ hoÃ¡n Ä‘á»•i vá»›i Ã´ sá»‘ liá»n ká».
+ Chi phÃ­
    - Má»—i bÆ°á»›c di chuyá»ƒn cÃ³ chi phÃ­ báº±ng 1, vÃ¬ bÃ i toÃ¡n Æ°u tiÃªn tÃ¬m Ä‘Æ°á»ng Ä‘i ngáº¯n nháº¥t.
+ Giáº£i phÃ¡p
    - DÃ£y cÃ¡c tráº¡ng thÃ¡i tá»« tráº¡ng thÃ¡i ban Ä‘áº§u Ä‘áº¿n tráº¡ng thÃ¡i má»¥c tiÃªu, Ä‘Æ°á»£c táº¡o ra bá»Ÿi cÃ¡c: Thuáº­t toÃ¡n tÃ¬m kiáº¿m cá»¥c bá»™ (Q-learning, SARSA, Deep Q-Network, Policy-gradient).
### 2.6.1. Q-learning
Q-Learning lÃ  má»™t thuáº­t toÃ¡n há»c tÄƒng cÆ°á»ng (Reinforcement Learning) khÃ´ng cáº§n mÃ´ hÃ¬nh (model-free), dÃ¹ng Ä‘á»ƒ tÃ¬m chÃ­nh sÃ¡ch tá»‘i Æ°u trong má»™t mÃ´i trÆ°á»ng. Má»¥c tiÃªu lÃ  há»c cÃ¡ch chá»n hÃ nh Ä‘á»™ng tá»‘i Æ°u trong má»—i tráº¡ng thÃ¡i Ä‘á»ƒ tá»‘i Ä‘a hÃ³a pháº§n thÆ°á»Ÿng tÃ­ch lÅ©y vá» sau.
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng trong Q-learing
  - Q-table (dict hoáº·c 2D array) : LÆ°u giÃ¡ trá»‹ Q(s, a) cho tá»«ng cáº·p tráº¡ng thÃ¡i â€“ hÃ nh Ä‘á»™ng
  - List / Set: LÆ°u danh sÃ¡ch cÃ¡c tráº¡ng thÃ¡i vÃ  hÃ nh Ä‘á»™ng há»£p lá»‡
  - Tuple: DÃ¹ng lÃ m khÃ³a (s, a) náº¿u Q-table lÃ  tá»« Ä‘iá»ƒn
  - Random	: DÃ¹ng Ä‘á»ƒ chá»n hÃ nh Ä‘á»™ng ngáº«u nhiÃªn (exploration)
+ HÃ¬nh áº£nh minh há»a
### 2.6.2. SARSA
SARSA (Stateâ€“Actionâ€“Rewardâ€“Stateâ€“Action) lÃ  má»™t thuáº­t toÃ¡n há»c tÄƒng cÆ°á»ng (Reinforcement Learning) cÃ³ mÃ´ hÃ¬nh chÃ­nh sÃ¡ch (on-policy), dÃ¹ng Ä‘á»ƒ há»c chÃ­nh sÃ¡ch tá»‘i Æ°u thÃ´ng qua tÆ°Æ¡ng tÃ¡c vá»›i mÃ´i trÆ°á»ng. NÃ³ tÆ°Æ¡ng tá»± Q-learning, nhÆ°ng khÃ¡c á»Ÿ cÃ¡ch cáº­p nháº­t giÃ¡ trá»‹ Q.
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng
  - Q-table (dict hoáº·c 2D array): LÆ°u Q(s, a) â€” giÃ¡ trá»‹ hÃ nh Ä‘á»™ng táº¡i tráº¡ng thÃ¡i
  - Tuple: LÃ m khÃ³a (s, a) cho Q-table
  - List/Set: Danh sÃ¡ch tráº¡ng thÃ¡i vÃ  hÃ nh Ä‘á»™ng há»£p lá»‡
  - Random: Chá»n hÃ nh Ä‘á»™ng ngáº«u nhiÃªn (Îµ-greedy exploration)
+ HÃ¬nh áº£nh minh há»a
### 2.6.3. Deep Q-Network
Deep Q-Network (DQN) lÃ  má»™t phiÃªn báº£n má»Ÿ rá»™ng cá»§a Q-Learning dÃ¹ng máº¡ng nÆ¡-ron sÃ¢u (Deep Neural Network) Ä‘á»ƒ xáº¥p xá»‰ hÃ m Q(s, a) thay vÃ¬ dÃ¹ng báº£ng Q-table nhÆ° trong Q-Learning cá»• Ä‘iá»ƒn.
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng trong Deep Q-Network
  - Neural Network: Xáº¥p xá»‰ hÃ m Q(s, a), input lÃ  tráº¡ng thÃ¡i, output lÃ  vector Q cho má»i hÃ nh Ä‘á»™ng
  - Replay Buffer (Deque/List): LÆ°u trá»¯ cÃ¡c tráº£i nghiá»‡m (s, a, r, s') Ä‘á»ƒ huáº¥n luyá»‡n láº¡i vÃ  trÃ¡nh overfitting
  - Tuple: LÆ°u cÃ¡c máº«u (state, action, reward, next_state) trong replay buffer
  - Target Network: Báº£n sao cá»§a máº¡ng Q hiá»‡n táº¡i, cáº­p nháº­t cháº­m hÆ¡n Ä‘á»ƒ á»•n Ä‘á»‹nh há»c
  - Mini-batch: Láº¥y ngáº«u nhiÃªn tá»« replay buffer Ä‘á»ƒ cáº­p nháº­t gradient
+ HÃ¬nh áº£nh minh há»a
### 2.6.4. Policy-gradient
Policy Gradient lÃ  má»™t nhÃ³m thuáº­t toÃ¡n trong Há»c tÄƒng cÆ°á»ng (Reinforcement Learning) nháº±m tá»‘i Æ°u trá»±c tiáº¿p chÃ­nh sÃ¡ch hÃ nh Ä‘á»™ng (policy) thÃ´ng qua gradient ascent. Thay vÃ¬ Æ°á»›c lÆ°á»£ng hÃ m giÃ¡ trá»‹ ğ‘„(ğ‘ ,ğ‘) nhÆ° Q-learning hay DQN, Policy Gradient há»c má»™t hÃ m xÃ¡c suáº¥t chá»n hÃ nh Ä‘á»™ng ğœ‹(ğ‘âˆ£ğ‘ ;ğœƒ) .
+ Cáº¥u trÃºc dá»¯ liá»‡u sá»­ dá»¥ng trong Policy-gradient
  - Neural Network: Äáº¡i diá»‡n cho chÃ­nh sÃ¡ch ( \pi(a
  - List/Tuple: LÆ°u trá»¯ cÃ¡c táº­p tráº£i nghiá»‡m (state, action, reward)
  - Trajectory (Episode): LÆ°u toÃ n bá»™ hÃ nh Ä‘á»™ng â€“ tráº¡ng thÃ¡i â€“ pháº§n thÆ°á»Ÿng cá»§a 1 láº§n chÆ¡i
  - Replay Buffer (tuá»³ chá»n): LÆ°u láº¡i cÃ¡c trajectory Ä‘á»ƒ huáº¥n luyá»‡n nhiá»u láº§n (Ã­t phá»• biáº¿n hÆ¡n DQN)
  - Gradient Optimizer: DÃ¹ng Ä‘á»ƒ cáº­p nháº­t trá»ng sá»‘ (Adam, SGD...)
+ HÃ¬nh áº£nh minh há»a





  
  
